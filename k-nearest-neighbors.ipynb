{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knn_title",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, roc_auc_score, \n",
    "                             roc_curve)\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed dataset\n",
    "df = pd.read_csv(\"./datasets/knn.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df.drop('Hypertension', axis=1) # Features\n",
    "y = df['Hypertension'] # Target\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(f\"set shape: {X_train.shape}\")\n",
    "print(f\"Class distribution: {y_train.value_counts()}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(f\"shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_baseline_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: baseline (with all features)\n",
    "baseline_knn = KNeighborsClassifier()\n",
    "baseline_knn.fit(X_train, y_train)\n",
    "\n",
    "# evaluate baseline model\n",
    "y_pred_baseline = baseline_knn.predict(X_test)\n",
    "y_prob_baseline = baseline_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1 Score': f1_score(y_test, y_pred_baseline),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_prob_baseline)\n",
    "}\n",
    "\n",
    "print(\"Baseline KNN Performance:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_baseline_metrics_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df = pd.DataFrame(list(baseline_metrics.items()), columns=['Metric', 'Score'])\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='Accent')\n",
    "plt.title('Model 1: Baseline KNN', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/knn_baseline_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_save_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the baseline model\n",
    "joblib.dump(baseline_knn, './results/models/knn_model_baseline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_tuned_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: tuned (with all features)\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 7, 9, 11, 13, 15, 17],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "cross_val = 5\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cross_val,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Sample from all features for tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_train, y_train, train_size=50000, random_state=42\n",
    ")\n",
    "\n",
    "# Perform tuning\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(\"\\nBest parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train model with best parameters\n",
    "tuned_knn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "tuned_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = tuned_knn.predict(X_test)\n",
    "y_prob_tuned = tuned_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned),\n",
    "    'Precision': precision_score(y_test, y_pred_tuned),\n",
    "    'Recall': recall_score(y_test, y_pred_tuned),\n",
    "    'F1 Score': f1_score(y_test, y_pred_tuned),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_prob_tuned)\n",
    "}\n",
    "\n",
    "print(\"\\nTuned KNN Performance:\")\n",
    "for metric, value in tuned_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_tuned_metrics_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tuned metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df = pd.DataFrame(list(tuned_metrics.items()), columns=['Metric', 'Score'])\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='Accent')\n",
    "plt.title('Model 2: Tuned KNN', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/knn_tuned_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: baseline vs tuned\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'],\n",
    "    'Baseline': [\n",
    "        baseline_metrics['Accuracy'],\n",
    "        baseline_metrics['Precision'],\n",
    "        baseline_metrics['Recall'],\n",
    "        baseline_metrics['F1 Score'],\n",
    "        baseline_metrics['AUC-ROC']\n",
    "    ],\n",
    "    'Tuned': [\n",
    "        tuned_metrics['Accuracy'],\n",
    "        tuned_metrics['Precision'],\n",
    "        tuned_metrics['Recall'],\n",
    "        tuned_metrics['F1 Score'],\n",
    "        tuned_metrics['AUC-ROC']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Comparison: Baseline vs Tuned\")\n",
    "print(comparison)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_melted = comparison.melt(id_vars='Metric', var_name='Model', value_name='Score')\n",
    "sns.barplot(data=comparison_melted, x='Metric', y='Score', hue='Model', palette='Accent')\n",
    "plt.title('Comparison: Baseline vs Tuned', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Evaluation Metric')\n",
    "plt.legend(title='Model Type')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/knn_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_underfitting_overfitting_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate underfitting/overfitting\n",
    "def evaluate_model_fit(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    gap = train_acc - test_acc\n",
    "    \n",
    "    # Determine fit status\n",
    "    if gap > 0.15:\n",
    "        status = \"Overfitting\"\n",
    "    elif train_acc < 0.7 and test_acc < 0.7:\n",
    "        status = \"Underfitting\"\n",
    "    else:\n",
    "        status = \"Good Fit\"\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Training Accuracy': [train_acc],\n",
    "        'Test Accuracy': [test_acc],\n",
    "        'Gap': [gap],\n",
    "        'Status': [status]\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_baseline_fit_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model fit\n",
    "baseline_fit = evaluate_model_fit(\n",
    "    baseline_knn, X_train, y_train, X_test, y_test, \n",
    "    \"Baseline KNN\"\n",
    ")\n",
    "print(\"Baseline Model Fit Analysis:\")\n",
    "print(baseline_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_tuned_fit_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model fit\n",
    "tuned_fit = evaluate_model_fit(\n",
    "    tuned_knn, X_train, y_train, X_test, y_test, \n",
    "    \"Tuned KNN\"\n",
    ")\n",
    "print(\"\\nTuned Model Fit Analysis:\")\n",
    "print(tuned_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_simple_gap_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gap visualization between training and test accuracy\n",
    "fit_results = pd.concat([baseline_fit, tuned_fit], ignore_index=True)\n",
    "\n",
    "# Create a simple bar chart showing the gap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=fit_results, x='Model', y='Gap', palette='Accent')\n",
    "plt.title('Gap Between Training and Test Accuracy', fontsize=14, pad=15)\n",
    "plt.ylabel('Accuracy Gap (Training - Test)')\n",
    "plt.xlabel('Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/knn_simple_gap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_simple_fit_status_chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fit status visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a bar plot with status colors\n",
    "status_colors = {'Good Fit': 'green', 'Overfitting': 'red', 'Underfitting': 'blue'}\n",
    "sns.barplot(data=fit_results, x='Model', y='Gap', hue='Status', \n",
    "            palette=status_colors, dodge=False)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=0.15, color='orange', linestyle='--', label='Overfitting Threshold')\n",
    "\n",
    "plt.title('Model Fit Status', fontsize=14, pad=15)\n",
    "plt.ylabel('Accuracy Gap (Training - Test)')\n",
    "plt.xlabel('Model')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/knn_fit_status.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Tuned Model\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low Hypertension', 'High Hypertension'],\n",
    "            yticklabels=['Low Hypertension', 'High Hypertension'])\n",
    "plt.title('Confusion Matrix - Tuned KNN')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig('./results/eda/knn_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_roc_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Tuned Model\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_tuned)\n",
    "auc_score = roc_auc_score(y_test, y_prob_tuned)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'KNN (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Tuned KNN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./results/eda/knn_roc_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_save_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save evaluation details\n",
    "model_name = \"KNN\" \n",
    "\n",
    "# create data frame for this model's metrics\n",
    "model_metrics = pd.DataFrame({\n",
    "    'Model': [\n",
    "        f'{model_name} - Baseline', \n",
    "        f'{model_name} - Tuned'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        baseline_metrics['Accuracy'],\n",
    "        tuned_metrics['Accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        baseline_metrics['Precision'],\n",
    "        tuned_metrics['Precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        baseline_metrics['Recall'],\n",
    "        tuned_metrics['Recall']\n",
    "    ],\n",
    "    'F1_Score': [\n",
    "        baseline_metrics['F1 Score'],\n",
    "        tuned_metrics['F1 Score']\n",
    "    ],\n",
    "    'AUC_ROC': [\n",
    "        baseline_metrics['AUC-ROC'],\n",
    "        tuned_metrics['AUC-ROC']\n",
    "    ]\n",
    "})\n",
    "\n",
    "#  save model details\n",
    "model_metrics.to_csv(f'./results/metrics/{model_name.lower().replace(\" \", \"_\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knn_save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "joblib.dump(tuned_knn, './results/models/knn_model_tuned.pkl')\n",
    "\n",
    "print(\"Completed...!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
