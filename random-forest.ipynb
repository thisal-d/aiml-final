{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "random_forest_title",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, roc_auc_score, \n",
    "                             roc_curve)\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed dataset\n",
    "df = pd.read_csv(\"./datasets/hypertension_dataset(encoded-balanced-feature_engineered).csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df.drop('Hypertension', axis=1) # features\n",
    "y = df['Hypertension'] # target\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# print the shape\n",
    "print(\"Training Data\")\n",
    "print(f\"set shape: {X_train.shape}\")\n",
    "print(f\"Class distribution: {y_train.value_counts()}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(f\"shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_baseline_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: baseline (with all features)\n",
    "baseline_rf = RandomForestClassifier(random_state=42)\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate baseline model\n",
    "y_pred_baseline = baseline_rf.predict(X_test)\n",
    "y_prob_baseline = baseline_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate metrics\n",
    "baseline_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1 Score': f1_score(y_test, y_pred_baseline),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_prob_baseline)\n",
    "}\n",
    "\n",
    "# print baseline metrics\n",
    "print(\"Baseline Random Forest Performance:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_baseline_metrics_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df = pd.DataFrame(list(baseline_metrics.items()), columns=['Metric', 'Score'])\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='Accent')\n",
    "plt.title('Model 1: Baseline Random Forest', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/random_forest_baseline_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_save_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the baseline model\n",
    "joblib.dump(baseline_rf, './results/models/random_forest_model_baseline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_tuned_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: tuned (with all features)\n",
    "# define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         # 2 options\n",
    "    'max_depth': [10, 20, None],        # 3 options\n",
    "    'min_samples_split': [2, 5],        # 2 options\n",
    "    'min_samples_leaf': [1, 2],         # 2 options\n",
    "    'max_features': ['sqrt', 'log2'],   # 2 options\n",
    "    'bootstrap': [True]                  # 1 option\n",
    "}\n",
    "\n",
    "cross_val = 5\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cross_val,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Sample from all features for tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_train, y_train, train_size=50000, random_state=42\n",
    ")\n",
    "\n",
    "# perform tuning\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(\"\\nBest parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train model with best parameters\n",
    "tuned_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)\n",
    "tuned_rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the tuned model\n",
    "y_pred_tuned = tuned_rf.predict(X_test)\n",
    "y_prob_tuned = tuned_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calculate metrics\n",
    "tuned_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned),\n",
    "    'Precision': precision_score(y_test, y_pred_tuned),\n",
    "    'Recall': recall_score(y_test, y_pred_tuned),\n",
    "    'F1 Score': f1_score(y_test, y_pred_tuned),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_prob_tuned)\n",
    "}\n",
    "\n",
    "print(\"\\nTuned Random Forest Performance:\")\n",
    "for metric, value in tuned_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_tuned_metrics_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tuned metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df = pd.DataFrame(list(tuned_metrics.items()), columns=['Metric', 'Score'])\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='Accent')\n",
    "plt.title('Model 2: Tuned Random Forest', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/random_forest_tuned_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison baseline vs tuned\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'],\n",
    "    'Baseline': [\n",
    "        baseline_metrics['Accuracy'],\n",
    "        baseline_metrics['Precision'],\n",
    "        baseline_metrics['Recall'],\n",
    "        baseline_metrics['F1 Score'],\n",
    "        baseline_metrics['AUC-ROC']\n",
    "    ],\n",
    "    'Tuned': [\n",
    "        tuned_metrics['Accuracy'],\n",
    "        tuned_metrics['Precision'],\n",
    "        tuned_metrics['Recall'],\n",
    "        tuned_metrics['F1 Score'],\n",
    "        tuned_metrics['AUC-ROC']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Comparison: Baseline vs Tuned\")\n",
    "print(comparison)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_melted = comparison.melt(id_vars='Metric', var_name='Model', value_name='Score')\n",
    "sns.barplot(data=comparison_melted, x='Metric', y='Score', hue='Model', palette='Accent')\n",
    "plt.title('Comparison: Baseline vs Tuned', fontsize=14, pad=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Evaluation Metric')\n",
    "plt.legend(title='Model Type')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/random_forest_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_underfitting_overfitting_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate underfitting/overfitting\n",
    "def evaluate_model_fit(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    gap = train_acc - test_acc\n",
    "    \n",
    "    # Determine fit status\n",
    "    if gap > 0.15:\n",
    "        status = \"Overfitting\"\n",
    "    elif train_acc < 0.7 and test_acc < 0.7:\n",
    "        status = \"Underfitting\"\n",
    "    else:\n",
    "        status = \"Good Fit\"\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Training Accuracy': [train_acc],\n",
    "        'Test Accuracy': [test_acc],\n",
    "        'Gap': [gap],\n",
    "        'Status': [status]\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_baseline_fit_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model fit\n",
    "baseline_fit = evaluate_model_fit(\n",
    "    baseline_rf, X_train, y_train, X_test, y_test, \n",
    "    \"Baseline Random Forest\"\n",
    ")\n",
    "print(\"Baseline Model Fit Analysis:\")\n",
    "print(baseline_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_tuned_fit_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model fit\n",
    "tuned_fit = evaluate_model_fit(\n",
    "    tuned_rf, X_train, y_train, X_test, y_test, \n",
    "    \"Tuned Random Forest\"\n",
    ")\n",
    "print(\"\\nTuned Model Fit Analysis:\")\n",
    "print(tuned_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_simple_gap_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gap visualization between training and test accuracy\n",
    "fit_results = pd.concat([baseline_fit, tuned_fit], ignore_index=True)\n",
    "\n",
    "# Create a simple bar chart showing the gap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=fit_results, x='Model', y='Gap', palette='Accent')\n",
    "plt.title('Gap Between Training and Test Accuracy', fontsize=14, pad=15)\n",
    "plt.ylabel('Accuracy Gap (Training - Test)')\n",
    "plt.xlabel('Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/random_forest_simple_gap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_simple_fit_status_chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fit status visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a bar plot with status colors\n",
    "status_colors = {'Good Fit': 'green', 'Overfitting': 'red', 'Underfitting': 'blue'}\n",
    "sns.barplot(data=fit_results, x='Model', y='Gap', hue='Status', \n",
    "            palette=status_colors, dodge=False)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=0.15, color='orange', linestyle='--', label='Overfitting Threshold')\n",
    "\n",
    "plt.title('Model Fit Status', fontsize=14, pad=15)\n",
    "plt.ylabel('Accuracy Gap (Training - Test)')\n",
    "plt.xlabel('Model')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/eda/random_forest_fit_status.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Tuned Model\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Low Hypertension', 'High Hypertension'],\n",
    "            yticklabels=['Low Hypertension', 'High Hypertension'])\n",
    "plt.title('Confusion Matrix - Tuned Random Forest')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig('./results/eda/random_forest_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_roc_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Tuned Model\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_tuned)\n",
    "auc_score = roc_auc_score(y_test, y_prob_tuned)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Tuned Random Forest')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./results/eda/random_forest_roc_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_save_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save evaluation details\n",
    "model_name = \"Random Forest\" \n",
    "\n",
    "# create data frame for this model's metrics\n",
    "model_metrics = pd.DataFrame({\n",
    "    'Model': [\n",
    "        f'{model_name} - Baseline', \n",
    "        f'{model_name} - Tuned'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        baseline_metrics['Accuracy'],\n",
    "        tuned_metrics['Accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        baseline_metrics['Precision'],\n",
    "        tuned_metrics['Precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        baseline_metrics['Recall'],\n",
    "        tuned_metrics['Recall']\n",
    "    ],\n",
    "    'F1_Score': [\n",
    "        baseline_metrics['F1 Score'],\n",
    "        tuned_metrics['F1 Score']\n",
    "    ],\n",
    "    'AUC_ROC': [\n",
    "        baseline_metrics['AUC-ROC'],\n",
    "        tuned_metrics['AUC-ROC']\n",
    "    ]\n",
    "})\n",
    "\n",
    "#  save model details\n",
    "model_metrics.to_csv(f'./results/metrics/{model_name.lower().replace(\" \", \"_\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest_save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "joblib.dump(tuned_rf, './results/models/random_forest_model_tuned.pkl')\n",
    "\n",
    "print(\"Completed...!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}